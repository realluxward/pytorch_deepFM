{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e02829-d645-4403-8988-824644914b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import time, json, datetime \n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246c180e-f287-48a6-a062-dd7b51cb4dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8, \n",
    "                 hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]): \n",
    "        \"\"\"\n",
    "        cate_fea_nuniqs: 类别特征的唯一值个数列表，也就是每个类别特征的vocab_size所组成的列表\n",
    "        nume_fea_size: 数值特征的个数，该模型会考虑到输入全为类别型，即没有数值特征的情况 \n",
    "        emb_size：嵌入层的纬度，也就是隐向量的k的值，emb层把原始的稀疏向量变成一个稠密的隐向量\n",
    "        hid_dims：隐藏层（也即全连接层的纬度？）256,128代表全连接层有两层，第一层有256个神经元，第二层12个\n",
    "        num_classes：默认等于1的话就是表示一个二分类问题，最后返回的结果就是一个标量，表示结果为1的概率\n",
    "        dropout：防止过拟合的方法，比如说有256个神经元，0.2表示会随机的丢掉百分之20的神经元，具体是用一个256维的向量有0.8的都是1，其他的是0，与输入的向量做内积，将得到的向量再去链接\n",
    "        0.2,0.2表示输入层会dropout0.2，每个隐藏层也是0.2\n",
    "        \n",
    "        self就是指自己，自己这个模型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cate_fea_size = len(cate_fea_nuniqs) # cate_fea_nuniqs是一个列表，里面是输入数据中类别型的特征列\n",
    "        self.nume_fea_size = nume_fea_size\n",
    "        \n",
    "        \"\"\"FM部分\"\"\"\n",
    "        # 一阶\n",
    "        if self.nume_fea_size != 0: # 如果有数值型的特征\n",
    "            self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1)  # 数值特征的一阶表示\n",
    "            \"\"\"\n",
    "            定义了一个linear层的变量，\n",
    "            传入两个变量，第一个是输入层的纬度，也就是输入的特征向量由多少列，第二个事输出层的纬度，也就是输出的向量有多少列\n",
    "            这个层会根据这两个数，设置一个权重矩阵，例如特征向量有5列，输出为1，那么就有一个1*5的矩阵，用bs*5的输入矩阵*（5*1）的权重矩阵再加上偏移矩阵（5*1）\n",
    "            那么经过线性层处理后的结果就是一个bs*1的矩阵\n",
    "            \"\"\"\n",
    "        self.fm_1st_order_sparse_emb = nn.ModuleList([\n",
    "            nn.Embedding(voc_size, 1) for voc_size in cate_fea_nuniqs])  # 类别特征的一阶表示\n",
    "        \"\"\"\n",
    "        self.fm_1st_order_sparse_emb是一个列表，其中的每个元素都是一个nn.Embedding对象\n",
    "        voc_size表示每个类别型特征的可能取值数量\n",
    "        nn.Embedding(voc_size, 1)函数创建一个nn.Embedding对象\n",
    "        \n",
    "        这个对象对一个列特征进行处理，原来的列是bs*1的，处理过后变成bs*1*1的，第一个1是由于只有一列所以得到1，后面的1是参数设定的输出一列\n",
    "        emb详解：假设原来是一个2*4的特征矩阵，有两个样本，4个特征，每个特征有10种类型，且emb的参数设置为10,3，那么经过emb之后，变成2*4*3的矩阵\n",
    "        这里的话，原来是bs*1的，emb之后变成bs*1*1的[[1][2]...[bs]]\n",
    "        每一个对象都会去给某一个特征列向量进行处理，可以看成针对每一个特征向量有一个自己的处理函数，所有的函数构成了一个列表\n",
    "        \"\"\"\n",
    "        \n",
    "        # 二阶\n",
    "        self.fm_2nd_order_sparse_emb = nn.ModuleList([\n",
    "            nn.Embedding(voc_size, emb_size) for voc_size in cate_fea_nuniqs])  # 类别特征的二阶表示\n",
    "        \"\"\"\n",
    "        同样是一个列表，每一项都是一个embedding对象，可以看做是一个emb方法，输入一个bs*1的特征列，得到一个bs*1*emb_size的矩阵\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        self.all_dims = [self.cate_fea_size * emb_size] + hid_dims # 合成一个新向量，第一个元素是品类特征数*emb_size,第二个第三个是输入层和中间隐藏层的神经元数\n",
    "        self.dense_linear = nn.Linear(self.nume_fea_size, self.cate_fea_size * emb_size)  # 数值特征的维度变换到FM输出维度一致\n",
    "        # Wx + b\n",
    "        self.relu = nn.ReLU()\n",
    "        # for DNN \n",
    "        for i in range(1, len(self.all_dims)):# self.all_dims=3,因此会从1到2做一个循环\n",
    "            setattr(self, 'linear_'+str(i), nn.Linear(self.all_dims[i-1], self.all_dims[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(self.all_dims[i]))\n",
    "            setattr(self, 'activation_' + str(i), nn.ReLU())\n",
    "            setattr(self, 'dropout_'+str(i), nn.Dropout(dropout[i-1]))\n",
    "        \"\"\"\n",
    "        给模型添加了四层，线性层，批量归一化层(nn.BatchNorm1d)、激活函数(nn.ReLU)和dropout层(nn.Dropout）\n",
    "        DNN部分还有更多的层，len(self.all_dims)=3，所以先添加一个叫\n",
    "        linear_1的线性层，输入纬度是cate_fea_size * emb_size，输出是256\n",
    "        batchNorm_1的批标准化层，这个层不会改变数据的纬度，是将每一个特征列的数据进行标准化和缩放，\n",
    "        activation_1的激活层，对输入的数据进行relu处理，不会改变数据维度\n",
    "        droupout_1层，第一个参数是0.2\n",
    "        同理还会再来一轮\n",
    "        linear_2。。。输入是256，也就是上一层的输出，输出是128\n",
    "        略\n",
    "        \"\"\"\n",
    "        # for output 输出层，一个线性层，\n",
    "        self.dnn_linear = nn.Linear(hid_dims[-1], num_classes)# 输出层的线行层，输入纬度是128，输出纬度是1\n",
    "        self.sigmoid = nn.Sigmoid() #输出的sigmoid层\n",
    "        \n",
    "    def forward(self, X_sparse, X_dense=None):\n",
    "        \"\"\"\n",
    "        X_sparse: 类别型特征输入  [bs, cate_fea_size]\n",
    "        X_dense: 数值型特征输入（可能没有）  [bs, dense_fea_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"FM 一阶部分\"\"\"\n",
    "        # X_sparse (bs, cate_fea_size) \n",
    "        fm_1st_sparse_res = [emb(X_sparse[:, i].unsqueeze(1)).view(-1, 1) \n",
    "                             for i, emb in enumerate(self.fm_1st_order_sparse_emb)]\n",
    "        \"\"\"\n",
    "        X_sparse[:, i]是一个[bs]的向量,用unsqueeze处理过后，变成[bs*1]的张量，即[[1][2]...[bs]]\n",
    "        然后用emb方法去对这个bs*1的向量进行处理，得到的就是bs*1*1的向量，用view处理，变成（bs*1）的列向量，[[1],[2],[3]...,[bs]]每一个特征列都进行一样的处理，\n",
    "        所以会得到一个(cate_fea_size*bs*1)的一个张量\n",
    "        x = x.unsqueeze(1)  # 在第二个维度上增加一个维度\n",
    "        .view(-1, 1) 是 PyTorch 中用于重新调整 tensor 形状的方法之一，它的作用是将 tensor 转换为一个列向量\n",
    "        \n",
    "        \"\"\"\n",
    "        fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim=1)  # [bs, cate_fea_size] # (B, Nc)\n",
    "        \"\"\"\n",
    "        用cat方法，将每一列合并,由原来的cate_fea_size*bs*1变成了bs*cate_fea_size的张量\n",
    "        \"\"\"\n",
    "        \n",
    "        fm_1st_sparse_res = torch.sum(fm_1st_sparse_res, 1,  keepdim=True)  # [bs, 1]\n",
    "        \"\"\"\n",
    "        sum函数，第一个参数是输入的矩阵，第二个是纬度，0表示对某一列的所有行加和，1表示对每一行的所有列加和，\n",
    "        第三个是是否保持原来的纬度输出，原来是bs*cate_fea_size，那么加和之后变成bs*1,维数不变（指没有变成（bs））\n",
    "        \"\"\"\n",
    "        \n",
    "        if X_dense is not None:\n",
    "            fm_1st_dense_res = self.fm_1st_order_dense(X_dense) \n",
    "            fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res\n",
    "        else:\n",
    "            fm_1st_part = fm_1st_sparse_res   # [bs, 1]\n",
    "        \"\"\"\n",
    "        调用了之前设定的线性层，输入一个bs*nume_fea_size的特征矩阵，经过线性层，输入矩阵会成上一个权重矩阵的转置（权重矩阵是1*nume_fea_size的）\n",
    "        得到一个bs*1的向量\n",
    "        如果有分类型的向量和数值型的向量，分别都是bs*1的，那么就各项求和，得到结果也是bs*1的\n",
    "        \"\"\"\n",
    "        \"\"\"FM 二阶部分\"\"\"\n",
    "        fm_2nd_order_res = [emb(X_sparse[:, i].unsqueeze(1)) for i, emb in enumerate(self.fm_2nd_order_sparse_emb)]\n",
    "        fm_2nd_concat_1d = torch.cat(fm_2nd_order_res, dim=1) # (bs,cate_fea_size,emb_size)\n",
    "        \"\"\"\n",
    "        首先取出X_sparse[:, i]，（bs）,然后unsqueeze之后变成（bs,1），经过emb操作，变成bs*1*emb_size的，\n",
    "        然后经过cat操作，在第二维进行合并，得到的就是bs*cate_fea_size*emb_size\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # 先求和再平方\n",
    "        sum_embed = torch.sum(fm_2nd_concat_1d, 1)  # [bs, emb_size]\n",
    "        square_sum_embed = sum_embed * sum_embed    # [bs, emb_size]\n",
    "        # 先平方再求和\n",
    "        square_embed = fm_2nd_concat_1d * fm_2nd_concat_1d  # [bs, n, emb_size]\n",
    "        sum_square_embed = torch.sum(square_embed, 1)  # [bs, emb_size]\n",
    "        # 相减除以2 \n",
    "        sub = square_sum_embed - sum_square_embed  \n",
    "        sub = sub * 0.5   # [bs, emb_size]\n",
    "        \n",
    "        fm_2nd_part = torch.sum(sub, 1, keepdim=True)   # [bs, 1]\n",
    "        \n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        dnn_out = torch.flatten(fm_2nd_concat_1d, 1)   #展开操作，原来是(bs,cate_fea_size,emb_size)，从第二维展开变成[bs,cate_fea_size * emb_size]\n",
    "        \n",
    "        \n",
    "        if X_dense is not None:\n",
    "            dense_out = self.relu(self.dense_linear(X_dense))   # [bs, n * emb_size]\n",
    "            \"\"\"\n",
    "            X_dense是一个[bs,nume_fea_size]的矩阵，经过上面的线性层，将变成[bs,cate_fea_size * emb_size]\n",
    "            经过relu层的处理后还是[bs,cate_fea_size * emb_size]\n",
    "            \"\"\"\n",
    "            dnn_out = dnn_out + dense_out   # [bs, n * emb_size]\n",
    "            \"\"\"\n",
    "            加号，直接数字对应位累加都是[bs,cate_fea_size * emb_size]的，加出来还是一个[bs,cate_fea_size * emb_size]的矩阵\n",
    "            \"\"\"\n",
    "        for i in range(1, len(self.all_dims)):\n",
    "            dnn_out = getattr(self, 'linear_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'batchNorm_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'activation_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'dropout_' + str(i))(dnn_out)\n",
    "        \"\"\"\n",
    "        依次用定义的四个层去处理\n",
    "        \"\"\"\n",
    "        dnn_out = self.dnn_linear(dnn_out)   # [bs, 1]\n",
    "        out = fm_1st_part + fm_2nd_part + dnn_out   # [bs, 1]\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36575a8-eb3d-4912-b347-40e8790c8ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096a421d-c774-42e8-b401-43564ce2a9b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e750f01e-53d9-4a0f-9528-3f1a7123e468",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 91.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/criteo_sample_50w.csv')\n",
    "\n",
    "dense_features = [f for f in data.columns.tolist() if f[0] == \"I\"]\n",
    "sparse_features = [f for f in data.columns.tolist() if f[0] == \"C\"]\n",
    "\"\"\"\n",
    "分别是两个向量，\n",
    "一个是数据型的向量，内容是数据类型向量的标题\n",
    "一个是类别型的向量，内容是类别类型向量的标题\n",
    "\"\"\"\n",
    "data[sparse_features] = data[sparse_features].fillna('-10086', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "target = ['label']\n",
    "\"\"\"\n",
    "对数据集中的缺失值进行填充\n",
    "\"\"\"\n",
    "## 类别特征labelencoder\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "## 数值特征标准化\n",
    "for feat in tqdm(dense_features):\n",
    "    mean = data[feat].mean()\n",
    "    std = data[feat].std() # 计算特征的标准差\n",
    "    data[feat] = (data[feat] - mean) / (std + 1e-12)   # 防止除零\n",
    "    \n",
    "\"\"\"\n",
    "把对数据进一步处理，\n",
    "首先是类别的数据：\n",
    "先实例化一个LabelEncoder对象，然后调用方法fit_transform方法，传入一个特征列向量，该方法会把类别用数字代替，然后返回列向量，更新\n",
    "上下的区别在于下面加了一个tqdm()方法,就是让这个过程一进度条的方式可视化出来，可以直接删掉，在大型数据处理中不会用，这个过程会造成损耗\n",
    "下面是数据类型的：目的是使得每个特征的均值为0，方差为1。\n",
    "先求每一个的均值，方差，然后计算处理\n",
    "\"\"\"\n",
    "pass\n",
    "# print(data.shape)\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fed7e14-17f2-4612-bd64-e91e89483b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droupout_1\n",
      "self.alldims:3\n",
      "droupout_2\n",
      "self.alldims:3\n",
      "{'Total': 7037251, 'Trainable': 7037251}\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "\"\"\"\n",
    "random_state是一个随机数的种子，用来随机分割数据集，当他为一个确定的数值的时候，就能够保证每次跑程序，数据集被分割的一样，是可复现的，\n",
    "\"\"\"\n",
    "# print(train.shape, valid.shape)\n",
    "\n",
    "train_dataset = Data.TensorDataset(torch.LongTensor(train[sparse_features].values), \n",
    "                                   torch.FloatTensor(train[dense_features].values),\n",
    "                                   torch.FloatTensor(train['label'].values),)\n",
    "\n",
    "\"\"\"\n",
    "训练集转化，讲训练集中的类别向量转换为LongTensor,数值向量转换为FloatTensor,label是点击次数，也作为FloatTensor,然后合并\n",
    "\"\"\"\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=2048, shuffle=True)\n",
    "\"\"\"\n",
    "得到了训练集进一步处理，将所有的训练数据变成一个一个的batch，shuffle是指每一个epoch都会重新随机分配数据形成新的batch组\n",
    "\n",
    "下面对测试集的数据也是同样的处理方法\n",
    "\"\"\"\n",
    "valid_dataset = Data.TensorDataset(torch.LongTensor(valid[sparse_features].values), \n",
    "                                   torch.FloatTensor(valid[dense_features].values),\n",
    "                                   torch.FloatTensor(valid['label'].values),)\n",
    "valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=4096, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "# print(device)\n",
    "cate_fea_nuniqs = [data[f].nunique() for f in sparse_features]\n",
    "\n",
    "\"\"\"\n",
    "首先sparse_features是类别向量的列标题，因此是循环每一个类别向量，\n",
    "cate_fea_nuniqs 是一个向量，每一个元素是每一个类别向量的不同的元素的个数，表示这一列有多少种不同的类别\n",
    "\"\"\"\n",
    "\n",
    "model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))\n",
    "\"\"\"\n",
    "声明一个deepFM对象，需要传入刚刚得到的，一个每个类别特征所包含的不同的种类数的向量，一个数值类型的特征数\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model.to(device)# 把模型放到设备上\n",
    "\n",
    "\n",
    "loss_fcn = nn.BCELoss()  # Loss函数\n",
    "loss_fcn = loss_fcn.to(device) # 将损失函数转移到指定的设备上\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\"\"\"\n",
    "第一个是模型的参数\n",
    "lr学习率，weight_decay是l2正则化的系数\n",
    "整个optimizer是基于梯度下降算法的优化器\n",
    "\"\"\"\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\"\"\"\n",
    "定义一个学习率调度器，optimizer是优化器，step_size是更新学习率的间隔，=1表示每一个epoch就更新一次，gamma是倍率，即每次更新，学习率都乘上0.8\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 打印模型参数\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n",
    "print(get_parameter_number(model))\n",
    "\n",
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "def write_log(w):\n",
    "    file_name = 'data/' + datetime.date.today().strftime('%m%d')+\"_{}.log\".format(\"deepfm\")\n",
    "    t0 = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f: \n",
    "        f.write(info + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a2b36-bd02-4ee9-b5e2-8673a0f91907",
   "metadata": {},
   "source": [
    "## todo\n",
    "- python magic function\n",
    "- 本地conda+python+vscode源码阅读环境\n",
    "- Module,dataset, dataloader借口源代码\n",
    "- 基于以上源码理顺DeepFM demo的逻辑 (数据的流转，shape变化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decfe37-bc55-41c1-a6a1-08e3769280d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ced707-f962-48fc-bf4e-6b310041faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr : 0.005\n",
      "15:44:11 : Epoch: 1\n",
      "15:44:14 : Epoch 0001 | Step 0050 / 196 | Loss 21.3312 | Time 3.3651\n",
      "15:44:18 : Epoch 0001 | Step 0100 / 196 | Loss 17.3678 | Time 6.7319\n",
      "15:44:21 : Epoch 0001 | Step 0150 / 196 | Loss 14.8405 | Time 10.1501\n",
      "15:44:24 : Epoch 0001 | Step 0196 / 196 | Loss 12.7069 | Time 13.1242\n",
      "15:44:26 : Current AUC: 0.651891, Best AUC: 0.651891\n",
      "\n",
      "Current lr : 0.004\n",
      "15:44:26 : Epoch: 2\n",
      "15:44:29 : Epoch 0002 | Step 0050 / 196 | Loss 2.8189 | Time 3.3366\n",
      "15:44:33 : Epoch 0002 | Step 0100 / 196 | Loss 2.2500 | Time 6.6121\n",
      "15:44:36 : Epoch 0002 | Step 0150 / 196 | Loss 1.8938 | Time 10.2096\n",
      "15:44:39 : Epoch 0002 | Step 0196 / 196 | Loss 1.6748 | Time 13.2173\n",
      "15:44:41 : Current AUC: 0.667140, Best AUC: 0.667140\n",
      "\n",
      "Current lr : 0.0032\n",
      "15:44:41 : Epoch: 3\n",
      "15:44:45 : Epoch 0003 | Step 0050 / 196 | Loss 0.7895 | Time 3.5865\n",
      "15:44:48 : Epoch 0003 | Step 0100 / 196 | Loss 0.7516 | Time 6.9783\n",
      "15:44:52 : Epoch 0003 | Step 0150 / 196 | Loss 0.7217 | Time 10.4598\n",
      "15:44:55 : Epoch 0003 | Step 0196 / 196 | Loss 0.6978 | Time 13.8507\n",
      "15:44:57 : Current AUC: 0.719235, Best AUC: 0.719235\n",
      "\n",
      "Current lr : 0.00256\n",
      "15:44:57 : Epoch: 4\n",
      "15:45:01 : Epoch 0004 | Step 0050 / 196 | Loss 0.5632 | Time 3.8537\n",
      "15:45:04 : Epoch 0004 | Step 0100 / 196 | Loss 0.5613 | Time 7.2294\n",
      "15:45:08 : Epoch 0004 | Step 0150 / 196 | Loss 0.5566 | Time 10.5853\n",
      "15:45:11 : Epoch 0004 | Step 0196 / 196 | Loss 0.5510 | Time 13.6339\n",
      "15:45:13 : Current AUC: 0.729497, Best AUC: 0.729497\n",
      "\n",
      "Current lr : 0.0020480000000000003\n",
      "15:45:13 : Epoch: 5\n",
      "15:45:16 : Epoch 0005 | Step 0050 / 196 | Loss 0.5093 | Time 3.4061\n",
      "15:45:19 : Epoch 0005 | Step 0100 / 196 | Loss 0.5099 | Time 6.8530\n",
      "15:45:23 : Epoch 0005 | Step 0150 / 196 | Loss 0.5100 | Time 10.1671\n",
      "15:45:26 : Epoch 0005 | Step 0196 / 196 | Loss 0.5085 | Time 13.2444\n",
      "15:45:28 : Current AUC: 0.748934, Best AUC: 0.748934\n",
      "\n",
      "Current lr : 0.0016384000000000004\n",
      "15:45:28 : Epoch: 6\n",
      "15:45:31 : Epoch 0006 | Step 0050 / 196 | Loss 0.4851 | Time 3.4123\n",
      "15:45:35 : Epoch 0006 | Step 0100 / 196 | Loss 0.4870 | Time 6.8841\n",
      "15:45:38 : Epoch 0006 | Step 0150 / 196 | Loss 0.4886 | Time 10.3758\n",
      "15:45:41 : Epoch 0006 | Step 0196 / 196 | Loss 0.4886 | Time 13.5455\n",
      "15:45:43 : Current AUC: 0.753594, Best AUC: 0.753594\n",
      "\n",
      "Current lr : 0.0013107200000000005\n",
      "15:45:43 : Epoch: 7\n",
      "15:45:47 : Epoch 0007 | Step 0050 / 196 | Loss 0.4762 | Time 3.2745\n",
      "15:45:50 : Epoch 0007 | Step 0100 / 196 | Loss 0.4784 | Time 6.6857\n",
      "15:45:54 : Epoch 0007 | Step 0150 / 196 | Loss 0.4803 | Time 10.1531\n",
      "15:45:57 : Epoch 0007 | Step 0196 / 196 | Loss 0.4806 | Time 13.2833\n",
      "15:45:59 : Current AUC: 0.760920, Best AUC: 0.760920\n",
      "\n",
      "Current lr : 0.0010485760000000005\n",
      "15:45:59 : Epoch: 8\n",
      "15:46:02 : Epoch 0008 | Step 0050 / 196 | Loss 0.4742 | Time 3.3879\n",
      "15:46:06 : Epoch 0008 | Step 0100 / 196 | Loss 0.4755 | Time 6.7912\n",
      "15:46:09 : Epoch 0008 | Step 0150 / 196 | Loss 0.4751 | Time 10.2002\n",
      "15:46:12 : Epoch 0008 | Step 0196 / 196 | Loss 0.4749 | Time 13.1714\n",
      "15:46:14 : Current AUC: 0.763985, Best AUC: 0.763985\n",
      "\n",
      "Current lr : 0.0008388608000000005\n",
      "15:46:14 : Epoch: 9\n",
      "15:46:17 : Epoch 0009 | Step 0050 / 196 | Loss 0.4666 | Time 3.3391\n",
      "15:46:20 : Epoch 0009 | Step 0100 / 196 | Loss 0.4689 | Time 6.5769\n",
      "15:46:24 : Epoch 0009 | Step 0150 / 196 | Loss 0.4704 | Time 9.8191\n",
      "15:46:27 : Epoch 0009 | Step 0196 / 196 | Loss 0.4705 | Time 12.8855\n",
      "15:46:28 : Current AUC: 0.767137, Best AUC: 0.767137\n",
      "\n",
      "Current lr : 0.0006710886400000004\n",
      "15:46:28 : Epoch: 10\n",
      "15:46:32 : Epoch 0010 | Step 0050 / 196 | Loss 0.4618 | Time 3.3879\n",
      "15:46:35 : Epoch 0010 | Step 0100 / 196 | Loss 0.4648 | Time 6.7372\n",
      "15:46:39 : Epoch 0010 | Step 0150 / 196 | Loss 0.4642 | Time 10.0557\n",
      "15:46:42 : Epoch 0010 | Step 0196 / 196 | Loss 0.4655 | Time 13.2467\n",
      "15:46:44 : Current AUC: 0.772196, Best AUC: 0.772196\n",
      "\n",
      "Current lr : 0.0005368709120000003\n",
      "15:46:44 : Epoch: 11\n",
      "15:46:47 : Epoch 0011 | Step 0050 / 196 | Loss 0.4589 | Time 3.5092\n",
      "15:46:51 : Epoch 0011 | Step 0100 / 196 | Loss 0.4604 | Time 6.8776\n",
      "15:46:54 : Epoch 0011 | Step 0150 / 196 | Loss 0.4613 | Time 10.1666\n",
      "15:46:57 : Epoch 0011 | Step 0196 / 196 | Loss 0.4621 | Time 13.2444\n",
      "15:46:59 : Current AUC: 0.771493, Best AUC: 0.772196\n",
      "\n",
      "Current lr : 0.0004294967296000003\n",
      "15:46:59 : Epoch: 12\n",
      "15:47:02 : Epoch 0012 | Step 0050 / 196 | Loss 0.4532 | Time 3.4511\n",
      "15:47:06 : Epoch 0012 | Step 0100 / 196 | Loss 0.4561 | Time 6.8658\n",
      "15:47:09 : Epoch 0012 | Step 0150 / 196 | Loss 0.4566 | Time 10.2268\n",
      "15:47:12 : Epoch 0012 | Step 0196 / 196 | Loss 0.4579 | Time 13.2381\n",
      "15:47:14 : Current AUC: 0.774106, Best AUC: 0.774106\n",
      "\n",
      "Current lr : 0.00034359738368000027\n",
      "15:47:14 : Epoch: 13\n",
      "15:47:17 : Epoch 0013 | Step 0050 / 196 | Loss 0.4463 | Time 3.3818\n",
      "15:47:20 : Epoch 0013 | Step 0100 / 196 | Loss 0.4500 | Time 6.6608\n",
      "15:47:24 : Epoch 0013 | Step 0150 / 196 | Loss 0.4512 | Time 10.1438\n",
      "15:47:27 : Epoch 0013 | Step 0196 / 196 | Loss 0.4532 | Time 13.2831\n",
      "15:47:29 : Current AUC: 0.777856, Best AUC: 0.777856\n",
      "\n",
      "Current lr : 0.00027487790694400024\n",
      "15:47:29 : Epoch: 14\n",
      "15:47:32 : Epoch 0014 | Step 0050 / 196 | Loss 0.4432 | Time 3.3976\n",
      "15:47:36 : Epoch 0014 | Step 0100 / 196 | Loss 0.4435 | Time 6.7945\n",
      "15:47:39 : Epoch 0014 | Step 0150 / 196 | Loss 0.4461 | Time 10.1639\n",
      "15:47:42 : Epoch 0014 | Step 0196 / 196 | Loss 0.4477 | Time 13.1564\n",
      "15:47:44 : Current AUC: 0.777992, Best AUC: 0.777992\n",
      "\n",
      "Current lr : 0.0002199023255552002\n",
      "15:47:44 : Epoch: 15\n",
      "15:47:47 : Epoch 0015 | Step 0050 / 196 | Loss 0.4354 | Time 3.2945\n",
      "15:47:51 : Epoch 0015 | Step 0100 / 196 | Loss 0.4370 | Time 6.6937\n",
      "15:47:54 : Epoch 0015 | Step 0150 / 196 | Loss 0.4391 | Time 10.0387\n",
      "15:47:57 : Epoch 0015 | Step 0196 / 196 | Loss 0.4410 | Time 13.0880\n",
      "15:47:59 : Current AUC: 0.779251, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 0.00017592186044416018\n",
      "15:47:59 : Epoch: 16\n",
      "15:48:03 : Epoch 0016 | Step 0050 / 196 | Loss 0.4262 | Time 3.4762\n",
      "15:48:06 : Epoch 0016 | Step 0100 / 196 | Loss 0.4273 | Time 6.8652\n",
      "15:48:09 : Epoch 0016 | Step 0150 / 196 | Loss 0.4290 | Time 10.2182\n",
      "15:48:13 : Epoch 0016 | Step 0196 / 196 | Loss 0.4308 | Time 13.4141\n",
      "15:48:14 : Current AUC: 0.776934, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 0.00014073748835532815\n",
      "15:48:14 : Epoch: 17\n",
      "15:48:18 : Epoch 0017 | Step 0050 / 196 | Loss 0.4080 | Time 3.3628\n",
      "15:48:21 : Epoch 0017 | Step 0100 / 196 | Loss 0.4110 | Time 6.7033\n",
      "15:48:24 : Epoch 0017 | Step 0150 / 196 | Loss 0.4127 | Time 10.0328\n",
      "15:48:27 : Epoch 0017 | Step 0196 / 196 | Loss 0.4152 | Time 13.0397\n",
      "15:48:29 : Current AUC: 0.772532, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 0.00011258999068426252\n",
      "15:48:29 : Epoch: 18\n",
      "15:48:32 : Epoch 0018 | Step 0050 / 196 | Loss 0.3833 | Time 3.2785\n",
      "15:48:36 : Epoch 0018 | Step 0100 / 196 | Loss 0.3841 | Time 6.7014\n",
      "15:48:39 : Epoch 0018 | Step 0150 / 196 | Loss 0.3865 | Time 10.0773\n",
      "15:48:42 : Epoch 0018 | Step 0196 / 196 | Loss 0.3885 | Time 13.3032\n",
      "15:48:44 : Current AUC: 0.762300, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 9.007199254741002e-05\n",
      "15:48:44 : Epoch: 19\n",
      "15:48:48 : Epoch 0019 | Step 0050 / 196 | Loss 0.3528 | Time 3.6303\n",
      "15:48:51 : Epoch 0019 | Step 0100 / 196 | Loss 0.3548 | Time 6.9558\n",
      "15:48:55 : Epoch 0019 | Step 0150 / 196 | Loss 0.3572 | Time 10.2889\n",
      "15:48:58 : Epoch 0019 | Step 0196 / 196 | Loss 0.3598 | Time 13.2688\n",
      "15:49:00 : Current AUC: 0.755012, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 7.205759403792802e-05\n",
      "15:49:00 : Epoch: 20\n",
      "15:49:03 : Epoch 0020 | Step 0050 / 196 | Loss 0.3322 | Time 3.3873\n",
      "15:49:06 : Epoch 0020 | Step 0100 / 196 | Loss 0.3346 | Time 6.6857\n",
      "15:49:10 : Epoch 0020 | Step 0150 / 196 | Loss 0.3359 | Time 10.0667\n",
      "15:49:13 : Epoch 0020 | Step 0196 / 196 | Loss 0.3381 | Time 13.1601\n",
      "15:49:14 : Current AUC: 0.750642, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 5.764607523034242e-05\n",
      "15:49:14 : Epoch: 21\n",
      "15:49:18 : Epoch 0021 | Step 0050 / 196 | Loss 0.3151 | Time 3.4776\n",
      "15:49:21 : Epoch 0021 | Step 0100 / 196 | Loss 0.3173 | Time 6.8392\n",
      "15:49:25 : Epoch 0021 | Step 0150 / 196 | Loss 0.3190 | Time 10.2404\n",
      "15:49:28 : Epoch 0021 | Step 0196 / 196 | Loss 0.3220 | Time 13.3266\n",
      "15:49:30 : Current AUC: 0.746699, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 4.611686018427394e-05\n",
      "15:49:30 : Epoch: 22\n",
      "15:49:33 : Epoch 0022 | Step 0050 / 196 | Loss 0.3044 | Time 3.3520\n",
      "15:49:36 : Epoch 0022 | Step 0100 / 196 | Loss 0.3054 | Time 6.7585\n",
      "15:49:40 : Epoch 0022 | Step 0150 / 196 | Loss 0.3068 | Time 10.0653\n",
      "15:49:43 : Epoch 0022 | Step 0196 / 196 | Loss 0.3083 | Time 13.1282\n",
      "15:49:45 : Current AUC: 0.741221, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 3.6893488147419155e-05\n",
      "15:49:45 : Epoch: 23\n",
      "15:49:48 : Epoch 0023 | Step 0050 / 196 | Loss 0.2962 | Time 3.3709\n",
      "15:49:51 : Epoch 0023 | Step 0100 / 196 | Loss 0.2976 | Time 6.7755\n",
      "15:49:55 : Epoch 0023 | Step 0150 / 196 | Loss 0.2978 | Time 10.1759\n",
      "15:49:58 : Epoch 0023 | Step 0196 / 196 | Loss 0.2988 | Time 13.1325\n",
      "15:50:00 : Current AUC: 0.738461, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 2.9514790517935324e-05\n",
      "15:50:00 : Epoch: 24\n",
      "15:50:03 : Epoch 0024 | Step 0050 / 196 | Loss 0.2887 | Time 3.4025\n",
      "15:50:06 : Epoch 0024 | Step 0100 / 196 | Loss 0.2907 | Time 6.7268\n",
      "15:50:10 : Epoch 0024 | Step 0150 / 196 | Loss 0.2906 | Time 10.1807\n",
      "15:50:13 : Epoch 0024 | Step 0196 / 196 | Loss 0.2919 | Time 13.1029\n",
      "15:50:15 : Current AUC: 0.739370, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 2.361183241434826e-05\n",
      "15:50:15 : Epoch: 25\n",
      "15:50:18 : Epoch 0025 | Step 0050 / 196 | Loss 0.2834 | Time 3.4446\n",
      "15:50:21 : Epoch 0025 | Step 0100 / 196 | Loss 0.2837 | Time 6.7989\n",
      "15:50:25 : Epoch 0025 | Step 0150 / 196 | Loss 0.2846 | Time 10.1023\n",
      "15:50:28 : Epoch 0025 | Step 0196 / 196 | Loss 0.2854 | Time 13.1764\n",
      "15:50:30 : Current AUC: 0.737103, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 1.888946593147861e-05\n",
      "15:50:30 : Epoch: 26\n",
      "15:50:33 : Epoch 0026 | Step 0050 / 196 | Loss 0.2791 | Time 3.4541\n",
      "15:50:36 : Epoch 0026 | Step 0100 / 196 | Loss 0.2787 | Time 6.7373\n",
      "15:50:40 : Epoch 0026 | Step 0150 / 196 | Loss 0.2805 | Time 10.0312\n",
      "15:50:43 : Epoch 0026 | Step 0196 / 196 | Loss 0.2811 | Time 13.0638\n",
      "15:50:44 : Current AUC: 0.735903, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 1.5111572745182888e-05\n",
      "15:50:44 : Epoch: 27\n",
      "15:50:48 : Epoch 0027 | Step 0050 / 196 | Loss 0.2780 | Time 3.3306\n",
      "15:50:51 : Epoch 0027 | Step 0100 / 196 | Loss 0.2777 | Time 6.6995\n",
      "15:50:54 : Epoch 0027 | Step 0150 / 196 | Loss 0.2775 | Time 9.9994\n",
      "15:50:57 : Epoch 0027 | Step 0196 / 196 | Loss 0.2780 | Time 13.0333\n",
      "15:50:59 : Current AUC: 0.732138, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 1.2089258196146311e-05\n",
      "15:50:59 : Epoch: 28\n",
      "15:51:02 : Epoch 0028 | Step 0050 / 196 | Loss 0.2735 | Time 3.3183\n",
      "15:51:06 : Epoch 0028 | Step 0100 / 196 | Loss 0.2744 | Time 6.5018\n",
      "15:51:09 : Epoch 0028 | Step 0150 / 196 | Loss 0.2746 | Time 9.9766\n",
      "15:51:12 : Epoch 0028 | Step 0196 / 196 | Loss 0.2755 | Time 12.9898\n",
      "15:51:14 : Current AUC: 0.731801, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 9.67140655691705e-06\n",
      "15:51:14 : Epoch: 29\n",
      "15:51:17 : Epoch 0029 | Step 0050 / 196 | Loss 0.2711 | Time 3.3753\n",
      "15:51:21 : Epoch 0029 | Step 0100 / 196 | Loss 0.2716 | Time 6.7778\n",
      "15:51:24 : Epoch 0029 | Step 0150 / 196 | Loss 0.2727 | Time 10.0920\n",
      "15:51:27 : Epoch 0029 | Step 0196 / 196 | Loss 0.2737 | Time 13.2162\n",
      "15:51:29 : Current AUC: 0.732174, Best AUC: 0.779251\n",
      "\n",
      "Current lr : 7.73712524553364e-06\n",
      "15:51:29 : Epoch: 30\n",
      "15:51:32 : Epoch 0030 | Step 0050 / 196 | Loss 0.2682 | Time 3.3206\n",
      "15:51:36 : Epoch 0030 | Step 0100 / 196 | Loss 0.2699 | Time 6.5883\n",
      "15:51:39 : Epoch 0030 | Step 0150 / 196 | Loss 0.2708 | Time 9.8509\n",
      "15:51:42 : Epoch 0030 | Step 0196 / 196 | Loss 0.2719 | Time 12.8451\n",
      "15:51:44 : Current AUC: 0.731950, Best AUC: 0.779251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_eval(model, train_loader, valid_loader, epochs, device):\n",
    "    best_auc = 0.0\n",
    "    for _ in range(epochs):\n",
    "        \"\"\"训练部分\"\"\"\n",
    "        model.train() #开启训练模式，在这个模式下，模型会使用dropout，batch，normalization等一些有利于模型的方法，model.eval()就是不会\n",
    "        print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        write_log('Epoch: {}'.format(_ + 1))\n",
    "        train_loss_sum = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        for idx, x in enumerate(train_loader):\n",
    "            cate_fea, nume_fea, label = x[0], x[1], x[2]\n",
    "            cate_fea, nume_fea, label = cate_fea.to(device), nume_fea.to(device), label.float().to(device)\n",
    "            pred = model(cate_fea, nume_fea).view(-1) # 将预测的结果展成一维\n",
    "            \"\"\"\n",
    "            这里是具体的开始训练\n",
    "            \"\"\"\n",
    "            loss = loss_fcn(pred, label) # 计算loss\n",
    "            optimizer.zero_grad() # 清零梯度\n",
    "            loss.backward() # 计算梯度\n",
    "            optimizer.step() #更新迭代器\n",
    "            \n",
    "            train_loss_sum += loss.cpu().item() # 累计一个epoch的loss，移动回cpu是为了节省不必要的开销，item是将向量的值提取出来变成一个标量\n",
    "            if (idx+1) % 50 == 0 or (idx + 1) == len(train_loader):\n",
    "                write_log(\"Epoch {:04d} | Step {:04d} / {} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          _+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start_time))\n",
    "        scheduler.step() # 更新学习率\n",
    "        \n",
    "        \n",
    "        \"\"\"推断部分\"\"\"\n",
    "        model.eval() #推断\n",
    "        with torch.no_grad(): #利用模型计算预测看效果，不需要再进行参数更新，因此这行代码临时禁止梯度计算\n",
    "            valid_labels, valid_preds = [], []\n",
    "            for idx, x in enumerate(valid_loader):\n",
    "                cate_fea, nume_fea, label = x[0], x[1], x[2]\n",
    "                cate_fea, nume_fea = cate_fea.to(device), nume_fea.to(device)\n",
    "                pred = model(cate_fea, nume_fea).reshape(-1).data.cpu().numpy().tolist()\n",
    "                valid_preds.extend(pred)\n",
    "                valid_labels.extend(label.cpu().numpy().tolist())\n",
    "        cur_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "        if cur_auc > best_auc:\n",
    "            best_auc = cur_auc\n",
    "            torch.save(model.state_dict(), \"data/deepfm_best.pth\")\n",
    "        write_log('Current AUC: %.6f, Best AUC: %.6f\\n' % (cur_auc, best_auc))\n",
    "        \n",
    "\n",
    "train_and_eval(model, train_loader, valid_loader, 30, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117698f-e72d-464f-8e58-e15ea0cd3989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58a4de-0772-489f-80c4-8721a1ced442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
